{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from FishDataset import FishDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from model import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FishDataset('../data', download=True, transform=train_transform, target_transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, test_indices = train_test_split(np.arange(len(train_dataset)), test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    sampler=SubsetRandomSampler(train_indices),\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    FishDataset('../data', transform=test_transform, target_transform=test_transform),\n",
    "    batch_size=32,\n",
    "    sampler=SubsetRandomSampler(train_indices),\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(outputs, targets):\n",
    "    outputs = outputs.view(outputs.size(0), -1)\n",
    "    targets = targets.view(targets.size(0), -1)\n",
    "    intersection = (outputs * targets).sum(1)\n",
    "    union = (outputs + targets).sum(1) - intersection\n",
    "    jac = (intersection + 0.001) / (union + 0.001)\n",
    "    return jac.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (down1): Sequential(\n",
       "    (0): conv_block(\n",
       "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "    (1): conv_block(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "  )\n",
       "  (down2): Sequential(\n",
       "    (0): conv_block(\n",
       "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "    (1): conv_block(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "  )\n",
       "  (down3): Sequential(\n",
       "    (0): conv_block(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "    (1): conv_block(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "  )\n",
       "  (middle): conv_block(\n",
       "    (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (leaky_relu): LeakyReLU(0.01)\n",
       "  )\n",
       "  (up3): Sequential(\n",
       "    (0): conv_block(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "    (1): conv_block(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "  )\n",
       "  (up2): Sequential(\n",
       "    (0): conv_block(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "    (1): conv_block(\n",
       "      (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "  )\n",
       "  (up1): Sequential(\n",
       "    (0): conv_block(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "    (1): conv_block(\n",
       "      (conv): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNet()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = os.path.abspath('../models')\n",
    "if not os.path.exists(model_folder):\n",
    "    os.mkdir(model_folder)\n",
    "model_path = os.path.join(model_folder, 'unet.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/5\n",
      "    batch   1/685 loss: 0.8074, jaccard 0.1583\n",
      "    batch  51/685 loss: 0.6603, jaccard 0.4997\n",
      "    batch 101/685 loss: 0.6422, jaccard 0.7225\n",
      "    batch 151/685 loss: 0.6436, jaccard 0.7481\n",
      "    batch 201/685 loss: 0.6349, jaccard 0.8381\n",
      "    batch 251/685 loss: 0.6301, jaccard 0.8434\n",
      "    batch 301/685 loss: 0.6293, jaccard 0.8403\n",
      "    batch 351/685 loss: 0.6259, jaccard 0.8785\n",
      "    batch 401/685 loss: 0.6274, jaccard 0.8671\n",
      "    batch 451/685 loss: 0.6249, jaccard 0.8744\n",
      "    batch 501/685 loss: 0.6268, jaccard 0.8539\n",
      "    batch 551/685 loss: 0.6261, jaccard 0.8535\n",
      "    batch 601/685 loss: 0.6237, jaccard 0.8710\n",
      "    batch 651/685 loss: 0.6235, jaccard 0.8817\n",
      "Finished epoch 1, starting evaluation\n",
      "    loss: 0.6359  jaccard: 1.5845 val_loss: 0.6234 val_jaccard: 0.8688\n",
      "\n",
      "Starting epoch 2/5\n",
      "    batch   1/685 loss: 0.6297, jaccard 0.8556\n",
      "    batch  51/685 loss: 0.6215, jaccard 0.8790\n",
      "    batch 101/685 loss: 0.6260, jaccard 0.8640\n",
      "    batch 151/685 loss: 0.6268, jaccard 0.8509\n",
      "    batch 201/685 loss: 0.6223, jaccard 0.8905\n",
      "    batch 251/685 loss: 0.6300, jaccard 0.8444\n",
      "    batch 301/685 loss: 0.6206, jaccard 0.8722\n",
      "    batch 351/685 loss: 0.6193, jaccard 0.8789\n",
      "    batch 401/685 loss: 0.6175, jaccard 0.8932\n",
      "    batch 451/685 loss: 0.6218, jaccard 0.8673\n",
      "    batch 501/685 loss: 0.6275, jaccard 0.8559\n",
      "    batch 551/685 loss: 0.6285, jaccard 0.8473\n",
      "    batch 601/685 loss: 0.6176, jaccard 0.8794\n",
      "    batch 651/685 loss: 0.6189, jaccard 0.8829\n",
      "Finished epoch 2, starting evaluation\n",
      "    loss: 0.6212  jaccard: 1.7561 val_loss: 0.6198 val_jaccard: 0.8825\n",
      "\n",
      "Starting epoch 3/5\n",
      "    batch   1/685 loss: 0.6144, jaccard 0.9039\n",
      "    batch  51/685 loss: 0.6200, jaccard 0.8896\n",
      "    batch 101/685 loss: 0.6167, jaccard 0.8924\n",
      "    batch 151/685 loss: 0.6209, jaccard 0.8745\n",
      "    batch 201/685 loss: 0.6198, jaccard 0.8951\n",
      "    batch 251/685 loss: 0.6314, jaccard 0.8603\n",
      "    batch 301/685 loss: 0.6175, jaccard 0.8838\n",
      "    batch 351/685 loss: 0.6158, jaccard 0.8955\n",
      "    batch 401/685 loss: 0.6208, jaccard 0.8917\n",
      "    batch 451/685 loss: 0.6170, jaccard 0.8873\n",
      "    batch 501/685 loss: 0.6137, jaccard 0.9063\n",
      "    batch 551/685 loss: 0.6233, jaccard 0.8718\n",
      "    batch 601/685 loss: 0.6195, jaccard 0.8894\n",
      "    batch 651/685 loss: 0.6180, jaccard 0.8886\n",
      "Finished epoch 3, starting evaluation\n",
      "    loss: 0.6182  jaccard: 1.7721 val_loss: 0.6170 val_jaccard: 0.8902\n",
      "\n",
      "Starting epoch 4/5\n",
      "    batch   1/685 loss: 0.6154, jaccard 0.9001\n",
      "    batch  51/685 loss: 0.6150, jaccard 0.8783\n",
      "    batch 101/685 loss: 0.6158, jaccard 0.8997\n",
      "    batch 151/685 loss: 0.6140, jaccard 0.8968\n",
      "    batch 201/685 loss: 0.6186, jaccard 0.8740\n",
      "    batch 251/685 loss: 0.6198, jaccard 0.8916\n",
      "    batch 301/685 loss: 0.6210, jaccard 0.8790\n",
      "    batch 351/685 loss: 0.6162, jaccard 0.8940\n",
      "    batch 401/685 loss: 0.6143, jaccard 0.8953\n",
      "    batch 451/685 loss: 0.6115, jaccard 0.9119\n",
      "    batch 501/685 loss: 0.6185, jaccard 0.8941\n",
      "    batch 551/685 loss: 0.6141, jaccard 0.8935\n",
      "    batch 601/685 loss: 0.6126, jaccard 0.8858\n",
      "    batch 651/685 loss: 0.6136, jaccard 0.8988\n",
      "Finished epoch 4, starting evaluation\n",
      "    loss: 0.6159  jaccard: 1.7813 val_loss: 0.6151 val_jaccard: 0.8883\n",
      "\n",
      "Starting epoch 5/5\n",
      "    batch   1/685 loss: 0.6157, jaccard 0.8964\n",
      "    batch  51/685 loss: 0.6155, jaccard 0.8906\n",
      "    batch 101/685 loss: 0.6119, jaccard 0.8941\n",
      "    batch 151/685 loss: 0.6158, jaccard 0.8976\n",
      "    batch 201/685 loss: 0.6171, jaccard 0.8858\n",
      "    batch 251/685 loss: 0.6161, jaccard 0.8713\n",
      "    batch 301/685 loss: 0.6161, jaccard 0.8865\n",
      "    batch 351/685 loss: 0.6153, jaccard 0.8830\n",
      "    batch 401/685 loss: 0.6100, jaccard 0.9113\n",
      "    batch 451/685 loss: 0.6205, jaccard 0.8807\n",
      "    batch 501/685 loss: 0.6107, jaccard 0.8846\n",
      "    batch 551/685 loss: 0.6105, jaccard 0.8963\n",
      "    batch 601/685 loss: 0.6122, jaccard 0.9112\n",
      "    batch 651/685 loss: 0.6099, jaccard 0.9065\n",
      "Finished epoch 5, starting evaluation\n",
      "    loss: 0.6137  jaccard: 1.7870 val_loss: 0.6128 val_jaccard: 0.8919\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist = {'loss': [], 'jaccard': [], 'val_loss': [], 'val_jaccard': []}\n",
    "num_epochs = 5\n",
    "display_steps = 50\n",
    "best_jaccard = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print('Starting epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "    # train\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_jaccard = 0.0\n",
    "    for batch_idx, (images, masks, _) in enumerate(train_loader):\n",
    "        images = Variable(images.cuda())\n",
    "        masks = Variable(masks.cuda())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        predicted = outputs.round()\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        jac = jaccard(outputs.round(), masks)\n",
    "        running_jaccard += jac.data[0]\n",
    "        \n",
    "        running_jaccard += jac.data[0]\n",
    "        running_loss += loss.data[0]\n",
    "        \n",
    "        if batch_idx % display_steps == 0:\n",
    "            print('    ', end='')\n",
    "            print('batch {:>3}/{:>3} loss: {:.4f}, jaccard {:.4f}\\r'.format(batch_idx+1, len(train_loader),\n",
    "                                                                     loss.data[0], jac.data[0]))\n",
    "\n",
    "        \n",
    "    # evalute\n",
    "    print('Finished epoch {}, starting evaluation'.format(epoch+1))\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_jaccard = 0.0\n",
    "    for images, masks, _ in val_loader:\n",
    "        images = Variable(images.cuda())\n",
    "        masks = Variable(masks.cuda())\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        val_running_loss += loss.data[0]\n",
    "        jac = jaccard(outputs.round(), masks)\n",
    "        val_running_jaccard += jac.data[0]\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_jaccard = running_jaccard / len(train_loader)\n",
    "    val_loss = val_running_loss / len(val_loader)\n",
    "    val_jaccard = val_running_jaccard / len(val_loader)\n",
    "    \n",
    "    hist['loss'].append(train_loss)\n",
    "    hist['jaccard'].append(train_jaccard)\n",
    "    hist['val_loss'].append(val_loss)\n",
    "    hist['val_jaccard'].append(val_jaccard)\n",
    "    \n",
    "    if val_jaccard > best_jaccard:\n",
    "        torch.save(model, model_path)\n",
    "    print('    ', end='')\n",
    "    print('loss: {:.4f}  jaccard: {:.4f} val_loss: {:.4f} val_jaccard: {:4.4f}\\n'.format(train_loss, train_jaccard,\n",
    "                                                                                         val_loss, val_jaccard))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
