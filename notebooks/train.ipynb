{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from FishDataset import FishDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from model import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FishDataset('../data', download=True, transform=train_transform, target_transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, test_indices = train_test_split(np.arange(len(train_dataset)), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    sampler=SubsetRandomSampler(train_indices),\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    FishDataset('../data', transform=test_transform, target_transform=test_transform),\n",
    "    batch_size=32,\n",
    "    sampler=SubsetRandomSampler(train_indices),\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(outputs, targets):\n",
    "    outputs = outputs.view(outputs.size(0), -1)\n",
    "    targets = targets.view(targets.size(0), -1)\n",
    "    intersection = (outputs * targets).sum(1)\n",
    "    union = (outputs + targets).sum(1) - intersection\n",
    "    jac = (intersection + 0.001) / (union + 0.001)\n",
    "    return jac.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (down1): Sequential(\n",
       "    (0): conv_block(\n",
       "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "    (1): conv_block(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "  )\n",
       "  (down2): Sequential(\n",
       "    (0): conv_block(\n",
       "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "    (1): conv_block(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "  )\n",
       "  (down3): Sequential(\n",
       "    (0): conv_block(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "    (1): conv_block(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "  )\n",
       "  (middle): conv_block(\n",
       "    (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (leaky_relu): LeakyReLU(0.01)\n",
       "  )\n",
       "  (up3): Sequential(\n",
       "    (0): conv_block(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "    (1): conv_block(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "  )\n",
       "  (up2): Sequential(\n",
       "    (0): conv_block(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "    (1): conv_block(\n",
       "      (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "  )\n",
       "  (up1): Sequential(\n",
       "    (0): conv_block(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "    (1): conv_block(\n",
       "      (conv): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNet()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = os.path.abspath('../models')\n",
    "if not os.path.exists(model_folder):\n",
    "    os.mkdir(model_folder)\n",
    "model_path = os.path.join(model_folder, 'unet.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/5\n",
      "    batch   1/685 loss: 0.7053, jaccard 0.0935\n",
      "    batch  51/685 loss: 0.6801, jaccard 0.7042\n",
      "    batch 101/685 loss: 0.6714, jaccard 0.7390\n",
      "    batch 151/685 loss: 0.6641, jaccard 0.7178\n",
      "    batch 201/685 loss: 0.6591, jaccard 0.7299\n",
      "    batch 251/685 loss: 0.6542, jaccard 0.7911\n",
      "    batch 301/685 loss: 0.6498, jaccard 0.8128\n",
      "    batch 351/685 loss: 0.6464, jaccard 0.8425\n",
      "    batch 401/685 loss: 0.6434, jaccard 0.8524\n",
      "    batch 451/685 loss: 0.6414, jaccard 0.8317\n",
      "    batch 501/685 loss: 0.6388, jaccard 0.8595\n",
      "    batch 551/685 loss: 0.6365, jaccard 0.8653\n",
      "    batch 601/685 loss: 0.6336, jaccard 0.8758\n",
      "    batch 651/685 loss: 0.6375, jaccard 0.7202\n",
      "Finished epoch 1, starting evaluation\n",
      "    loss: 0.6512  jaccard: 1.5415 val_loss: 0.6328 val_jaccard: 0.8725\n",
      "\n",
      "Starting epoch 2/5\n",
      "    batch   1/685 loss: 0.6337, jaccard 0.8684\n",
      "    batch  51/685 loss: 0.6294, jaccard 0.8859\n",
      "    batch 101/685 loss: 0.6302, jaccard 0.8603\n",
      "    batch 151/685 loss: 0.6320, jaccard 0.8496\n",
      "    batch 201/685 loss: 0.6281, jaccard 0.8850\n",
      "    batch 251/685 loss: 0.6322, jaccard 0.8571\n",
      "    batch 301/685 loss: 0.6285, jaccard 0.8529\n",
      "    batch 351/685 loss: 0.6257, jaccard 0.8782\n",
      "    batch 401/685 loss: 0.6268, jaccard 0.8826\n",
      "    batch 451/685 loss: 0.6257, jaccard 0.8760\n",
      "    batch 501/685 loss: 0.6265, jaccard 0.8998\n",
      "    batch 551/685 loss: 0.6260, jaccard 0.8787\n",
      "    batch 601/685 loss: 0.6235, jaccard 0.8938\n",
      "    batch 651/685 loss: 0.6224, jaccard 0.9137\n",
      "Finished epoch 2, starting evaluation\n",
      "    loss: 0.6277  jaccard: 1.7437 val_loss: 0.6231 val_jaccard: 0.8842\n",
      "\n",
      "Starting epoch 3/5\n",
      "    batch   1/685 loss: 0.6249, jaccard 0.8903\n",
      "    batch  51/685 loss: 0.6249, jaccard 0.8817\n",
      "    batch 101/685 loss: 0.6283, jaccard 0.8505\n",
      "    batch 151/685 loss: 0.6277, jaccard 0.8658\n",
      "    batch 201/685 loss: 0.6220, jaccard 0.8826\n",
      "    batch 251/685 loss: 0.6232, jaccard 0.8895\n",
      "    batch 301/685 loss: 0.6173, jaccard 0.9050\n",
      "    batch 351/685 loss: 0.6199, jaccard 0.8834\n",
      "    batch 401/685 loss: 0.6235, jaccard 0.8825\n",
      "    batch 451/685 loss: 0.6215, jaccard 0.8971\n",
      "    batch 501/685 loss: 0.6191, jaccard 0.8841\n",
      "    batch 551/685 loss: 0.6208, jaccard 0.8947\n",
      "    batch 601/685 loss: 0.6154, jaccard 0.9131\n",
      "    batch 651/685 loss: 0.6198, jaccard 0.8819\n",
      "Finished epoch 3, starting evaluation\n",
      "    loss: 0.6225  jaccard: 1.7770 val_loss: 0.6215 val_jaccard: 0.8911\n",
      "\n",
      "Starting epoch 4/5\n",
      "    batch   1/685 loss: 0.6224, jaccard 0.8983\n",
      "    batch  51/685 loss: 0.6266, jaccard 0.8776\n",
      "    batch 101/685 loss: 0.6215, jaccard 0.8822\n",
      "    batch 151/685 loss: 0.6218, jaccard 0.8827\n",
      "    batch 201/685 loss: 0.6177, jaccard 0.8999\n",
      "    batch 251/685 loss: 0.6164, jaccard 0.9042\n",
      "    batch 301/685 loss: 0.6168, jaccard 0.9024\n",
      "    batch 351/685 loss: 0.6208, jaccard 0.8855\n",
      "    batch 401/685 loss: 0.6134, jaccard 0.9165\n",
      "    batch 451/685 loss: 0.6239, jaccard 0.8761\n",
      "    batch 501/685 loss: 0.6165, jaccard 0.9043\n",
      "    batch 551/685 loss: 0.6199, jaccard 0.8710\n",
      "    batch 601/685 loss: 0.6187, jaccard 0.8951\n",
      "    batch 651/685 loss: 0.6206, jaccard 0.8748\n",
      "Finished epoch 4, starting evaluation\n",
      "    loss: 0.6199  jaccard: 1.7859 val_loss: 0.6188 val_jaccard: 0.8944\n",
      "\n",
      "Starting epoch 5/5\n",
      "    batch   1/685 loss: 0.6187, jaccard 0.8973\n",
      "    batch  51/685 loss: 0.6204, jaccard 0.8879\n",
      "    batch 101/685 loss: 0.6177, jaccard 0.8891\n",
      "    batch 151/685 loss: 0.6165, jaccard 0.9058\n",
      "    batch 201/685 loss: 0.6185, jaccard 0.8946\n",
      "    batch 251/685 loss: 0.6135, jaccard 0.9150\n",
      "    batch 301/685 loss: 0.6144, jaccard 0.8964\n",
      "    batch 351/685 loss: 0.6157, jaccard 0.9045\n",
      "    batch 401/685 loss: 0.6212, jaccard 0.8857\n",
      "    batch 451/685 loss: 0.6196, jaccard 0.8932\n",
      "    batch 501/685 loss: 0.6147, jaccard 0.9096\n",
      "    batch 551/685 loss: 0.6175, jaccard 0.9010\n",
      "    batch 601/685 loss: 0.6156, jaccard 0.9017\n",
      "    batch 651/685 loss: 0.6145, jaccard 0.8965\n",
      "Finished epoch 5, starting evaluation\n",
      "    loss: 0.6176  jaccard: 1.7917 val_loss: 0.6161 val_jaccard: 0.8980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist = {'loss': [], 'jaccard': [], 'val_loss': [], 'val_jaccard': []}\n",
    "num_epochs = 5\n",
    "display_steps = 50\n",
    "best_jaccard = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print('Starting epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "    # train\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_jaccard = 0.0\n",
    "    for batch_idx, (images, masks, _) in enumerate(train_loader):\n",
    "        images = Variable(images.cuda())\n",
    "        masks = Variable(masks.cuda())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        predicted = outputs.round()\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        jac = jaccard(outputs.round(), masks)\n",
    "        running_jaccard += jac.data[0]\n",
    "        \n",
    "        running_jaccard += jac.data[0]\n",
    "        running_loss += loss.data[0]\n",
    "        \n",
    "        if batch_idx % display_steps == 0:\n",
    "            print('    ', end='')\n",
    "            print('batch {:>3}/{:>3} loss: {:.4f}, jaccard {:.4f}\\r'.format(batch_idx+1, len(train_loader),\n",
    "                                                                     loss.data[0], jac.data[0]))\n",
    "\n",
    "        \n",
    "    # evalute\n",
    "    print('Finished epoch {}, starting evaluation'.format(epoch+1))\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_jaccard = 0.0\n",
    "    for images, masks, _ in val_loader:\n",
    "        images = Variable(images.cuda())\n",
    "        masks = Variable(masks.cuda())\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        val_running_loss += loss.data[0]\n",
    "        jac = jaccard(outputs.round(), masks)\n",
    "        val_running_jaccard += jac.data[0]\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_jaccard = running_jaccard / len(train_loader)\n",
    "    val_loss = val_running_loss / len(val_loader)\n",
    "    val_jaccard = val_running_jaccard / len(val_loader)\n",
    "    \n",
    "    hist['loss'].append(train_loss)\n",
    "    hist['jaccard'].append(train_jaccard)\n",
    "    hist['val_loss'].append(val_loss)\n",
    "    hist['val_jaccard'].append(val_jaccard)\n",
    "    \n",
    "    if val_jaccard > best_jaccard:\n",
    "        torch.save(model, model_path)\n",
    "    print('    ', end='')\n",
    "    print('loss: {:.4f}  jaccard: {:.4f} val_loss: {:.4f} val_jaccard: {:4.4f}\\n'.format(train_loss, train_jaccard,\n",
    "                                                                                         val_loss, val_jaccard))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
