{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from FishDataset import FishDataset\n",
    "from model import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FishDataset('../data', download=True, transform=train_transform, target_transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, test_indices = train_test_split(np.arange(len(train_dataset)), test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    sampler=SubsetRandomSampler(train_indices),\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    FishDataset('../data', transform=test_transform, target_transform=test_transform),\n",
    "    batch_size=32,\n",
    "    sampler=SubsetRandomSampler(train_indices),\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(outputs, targets):\n",
    "    outputs = outputs.view(outputs.size(0), -1)\n",
    "    targets = targets.view(targets.size(0), -1)\n",
    "    intersection = (outputs * targets).sum(1)\n",
    "    union = (outputs + targets).sum(1) - intersection\n",
    "    jac = (intersection + 0.001) / (union + 0.001)\n",
    "    return jac.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (down1): Sequential(\n",
       "    (0): conv_block(\n",
       "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "    (1): conv_block(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "  )\n",
       "  (down2): Sequential(\n",
       "    (0): conv_block(\n",
       "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "    (1): conv_block(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "  )\n",
       "  (down3): Sequential(\n",
       "    (0): conv_block(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "    (1): conv_block(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "  )\n",
       "  (down4): Sequential(\n",
       "    (0): conv_block(\n",
       "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "    (1): conv_block(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "  )\n",
       "  (middle): conv_block(\n",
       "    (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (leaky_relu): LeakyReLU(0.01)\n",
       "  )\n",
       "  (up4): Sequential(\n",
       "    (0): conv_block(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "    (1): conv_block(\n",
       "      (conv): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "  )\n",
       "  (up3): Sequential(\n",
       "    (0): conv_block(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "    (1): conv_block(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "  )\n",
       "  (up2): Sequential(\n",
       "    (0): conv_block(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "    (1): conv_block(\n",
       "      (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "  )\n",
       "  (up1): Sequential(\n",
       "    (0): conv_block(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "    (1): conv_block(\n",
       "      (conv): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNet()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/20\n",
      "loss: 0.7847\n",
      "loss: 0.6674\n",
      "loss: 0.6491\n",
      "loss: 0.6526\n",
      "loss: 0.6463\n",
      "loss: 0.6425\n",
      "loss: 0.6447\n",
      "loss: 0.6427\n",
      "loss: 0.6455\n",
      "loss: 0.6418\n",
      "loss: 0.6449\n",
      "loss: 0.6402\n",
      "loss: 0.6404\n",
      "loss: 0.6396\n",
      "loss: 0.6409\n",
      "loss: 0.6358\n",
      "loss: 0.6340\n",
      "loss: 0.6352\n",
      "loss: 0.6389\n",
      "loss: 0.6369\n",
      "loss: 0.6358\n",
      "loss: 0.6345\n",
      "loss: 0.6352\n",
      "loss: 0.6363\n",
      "loss: 0.6342\n",
      "loss: 0.6339\n",
      "loss: 0.6328\n",
      "loss: 0.6326\n",
      "loss: 0.6305\n",
      "loss: 0.6289\n",
      "loss: 0.6285\n",
      "loss: 0.6276\n",
      "loss: 0.6319\n",
      "loss: 0.6295\n",
      "loss: 0.6276\n",
      "loss: 0.6287\n",
      "loss: 0.6314\n",
      "loss: 0.6364\n",
      "loss: 0.6274\n",
      "loss: 0.6302\n",
      "loss: 0.6344\n",
      "loss: 0.6287\n",
      "loss: 0.6301\n",
      "loss: 0.6256\n",
      "loss: 0.6288\n",
      "loss: 0.6194\n",
      "loss: 0.6272\n",
      "loss: 0.6234\n",
      "loss: 0.6269\n",
      "loss: 0.6267\n",
      "loss: 0.6249\n",
      "loss: 0.6249\n",
      "loss: 0.6240\n",
      "loss: 0.6255\n",
      "loss: 0.6244\n",
      "loss: 0.6301\n",
      "loss: 0.6259\n",
      "loss: 0.6280\n",
      "loss: 0.6240\n",
      "loss: 0.6231\n",
      "loss: 0.6248\n",
      "loss: 0.6246\n",
      "loss: 0.6250\n",
      "loss: 0.6268\n",
      "loss: 0.6220\n",
      "loss: 0.6242\n",
      "loss: 0.6236\n",
      "loss: 0.6235\n",
      "loss: 0.6219\n",
      "Finished epoch 1, starting evaluation\n",
      "loss: 0.6336  val_loss: 0.6222 val_jaccard: 0.8742\n",
      "\n",
      "Starting epoch 2/20\n",
      "loss: 0.6231\n",
      "loss: 0.6234\n",
      "loss: 0.6202\n",
      "loss: 0.6236\n",
      "loss: 0.6211\n",
      "loss: 0.6198\n",
      "loss: 0.6282\n",
      "loss: 0.6243\n",
      "loss: 0.6238\n",
      "loss: 0.6202\n",
      "loss: 0.6224\n",
      "loss: 0.6283\n",
      "loss: 0.6251\n",
      "loss: 0.6226\n",
      "loss: 0.6252\n",
      "loss: 0.6194\n",
      "loss: 0.6262\n",
      "loss: 0.6214\n",
      "loss: 0.6205\n",
      "loss: 0.6209\n",
      "loss: 0.6226\n",
      "loss: 0.6215\n",
      "loss: 0.6191\n",
      "loss: 0.6219\n",
      "loss: 0.6214\n",
      "loss: 0.6229\n",
      "loss: 0.6197\n",
      "loss: 0.6201\n",
      "loss: 0.6185\n",
      "loss: 0.6210\n",
      "loss: 0.6182\n",
      "loss: 0.6199\n",
      "loss: 0.6250\n",
      "loss: 0.6188\n",
      "loss: 0.6195\n",
      "loss: 0.6230\n",
      "loss: 0.6238\n",
      "loss: 0.6262\n",
      "loss: 0.6214\n",
      "loss: 0.6248\n",
      "loss: 0.6157\n",
      "loss: 0.6223\n",
      "loss: 0.6186\n",
      "loss: 0.6243\n",
      "loss: 0.6230\n",
      "loss: 0.6155\n",
      "loss: 0.6207\n",
      "loss: 0.6198\n",
      "loss: 0.6192\n",
      "loss: 0.6186\n",
      "loss: 0.6199\n",
      "loss: 0.6142\n",
      "loss: 0.6244\n",
      "loss: 0.6192\n",
      "loss: 0.6227\n",
      "loss: 0.6203\n",
      "loss: 0.6159\n",
      "loss: 0.6175\n",
      "loss: 0.6191\n",
      "loss: 0.6174\n",
      "loss: 0.6201\n",
      "loss: 0.6211\n",
      "loss: 0.6180\n",
      "loss: 0.6212\n",
      "loss: 0.6156\n",
      "loss: 0.6208\n",
      "loss: 0.6246\n",
      "loss: 0.6169\n",
      "loss: 0.6236\n",
      "Finished epoch 2, starting evaluation\n",
      "loss: 0.6211  val_loss: 0.6191 val_jaccard: 0.8857\n",
      "\n",
      "Starting epoch 3/20\n",
      "loss: 0.6173\n",
      "loss: 0.6154\n",
      "loss: 0.6203\n",
      "loss: 0.6186\n",
      "loss: 0.6173\n",
      "loss: 0.6151\n",
      "loss: 0.6144\n",
      "loss: 0.6178\n",
      "loss: 0.6151\n",
      "loss: 0.6196\n",
      "loss: 0.6190\n",
      "loss: 0.6191\n",
      "loss: 0.6193\n",
      "loss: 0.6176\n",
      "loss: 0.6199\n",
      "loss: 0.6172\n",
      "loss: 0.6196\n",
      "loss: 0.6171\n",
      "loss: 0.6182\n",
      "loss: 0.6135\n",
      "loss: 0.6147\n",
      "loss: 0.6189\n",
      "loss: 0.6245\n",
      "loss: 0.6186\n",
      "loss: 0.6161\n",
      "loss: 0.6168\n",
      "loss: 0.6161\n",
      "loss: 0.6178\n",
      "loss: 0.6140\n",
      "loss: 0.6169\n",
      "loss: 0.6160\n",
      "loss: 0.6203\n",
      "loss: 0.6186\n",
      "loss: 0.6170\n",
      "loss: 0.6168\n",
      "loss: 0.6149\n",
      "loss: 0.6180\n",
      "loss: 0.6181\n",
      "loss: 0.6165\n",
      "loss: 0.6175\n",
      "loss: 0.6159\n"
     ]
    }
   ],
   "source": [
    "hist = {'loss': [], 'val_loss': [], 'val_jaccard': []}\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    print('Starting epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "    # train\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (images, masks, _) in enumerate(train_loader):\n",
    "        images = Variable(images.cuda())\n",
    "        masks = Variable(masks.cuda())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.data[0]\n",
    "        if batch_idx % 10 == 0:\n",
    "             print('loss: {:.4f}'.format(loss.data[0]))\n",
    "        \n",
    "    # evalute\n",
    "    print('Finished epoch {}, starting evaluation'.format(epoch+1))\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_jaccard = 0.0\n",
    "    for images, masks, _ in val_loader:\n",
    "        images = Variable(images.cuda())\n",
    "        masks = Variable(masks.cuda())\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        val_running_loss += loss.data[0]\n",
    "        predicted = outputs.data.round()\n",
    "        val_running_jaccard += jaccard(predicted, masks.data)\n",
    "    \n",
    "    loss = running_loss / len(train_loader)\n",
    "    val_loss = val_running_loss / len(val_loader)\n",
    "    val_jaccard = val_running_jaccard / len(val_loader)\n",
    "    \n",
    "    hist['loss'].append(loss)\n",
    "    hist['val_loss'].append(val_loss)\n",
    "    hist['val_jaccard'].append(val_jaccard)\n",
    "    \n",
    "    print('loss: {:.4f}  val_loss: {:.4f} val_jaccard: {:4.4f}\\n'.format(loss, val_loss, val_jaccard))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
